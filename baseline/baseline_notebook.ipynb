{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PEq_NKmEW9nW"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_data_loaders():\n",
    "    normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': True}\n",
    "\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                            transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                        transform=transform_test)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, **kwargs)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, **kwargs)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "#basicblock and resnet class\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1  = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1    = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ImageNet models\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2], 10)\n",
    "\n",
    "cfg = {\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, num):\n",
    "        super(VGG, self).__init__()\n",
    "        self.input_size = 32\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.n_maps = cfg[vgg_name][-2]\n",
    "        self.fc = self._make_fc_layers()\n",
    "        self.classifier = nn.Linear(self.n_maps, num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_fc_layers(self):\n",
    "        layers = []\n",
    "        layers += [nn.Linear(self.n_maps*self.input_size*self.input_size, self.n_maps),\n",
    "                   nn.BatchNorm1d(self.n_maps),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "                self.input_size = self.input_size // 2\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def VGG16():\n",
    "    return VGG('VGG16', 10)\n",
    "\n",
    "\n",
    "# map between model name and function\n",
    "models = {\n",
    "    'resnet18': ResNet18,\n",
    "    'vgg16': VGG16,\n",
    "}\n",
    "\n",
    "def load(model_name):\n",
    "    net = models[model_name]()\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "#initialize params\n",
    "def init_params(net):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal_(m.weight, std=1e-3)\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "\n",
    "# Training\n",
    "def train(trainloader, net, criterion, optimizer, use_cuda=True):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "                     \n",
    "            if optimizer_val=='sgd' or optimizer_val=='adam':\n",
    "              outputs = net(inputs)\n",
    "              loss = criterion(outputs, targets)\n",
    "              loss.backward()\n",
    "              optimizer.step()     \n",
    "\n",
    "            train_loss += loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "\n",
    "    return train_loss/total, 100 - 100.*correct/total,  100.*correct/total\n",
    "\n",
    "def test(testloader, net, criterion, use_cuda=True):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if isinstance(criterion, nn.CrossEntropyLoss):\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()*batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "\n",
    "    return test_loss/total, 100 - 100.*correct/total,  100.*correct/total\n",
    "\n",
    "\n",
    "def main(model_name, rand_seed, optimizer_val):\n",
    "    random.seed(rand_seed)\n",
    "    np.random.seed(rand_seed)\n",
    "    torch.manual_seed(rand_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(rand_seed)\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    trainloader, testloader = get_data_loaders()\n",
    "    net = ResNet18() if model_name == 'ResNet18' else VGG16()\n",
    "    init_params(net)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    if optimizer_val == 'sgd':\n",
    "        lr_val = 0.1\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr_val, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
    "    elif optimizer_val == 'adam':\n",
    "        lr_val = 0.001\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr_val, weight_decay=weight_decay)\n",
    "   \n",
    "    train_losses = np.zeros(epochs)\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss, train_err, train_acc = train(trainloader, net, criterion, optimizer, use_cuda)\n",
    "        train_losses[epoch] = loss\n",
    "        train_accuracy[epoch] = train_acc\n",
    "        test_loss, test_err, test_acc = test(testloader, net, criterion, use_cuda)\n",
    "        val_losses[epoch] = test_loss\n",
    "        val_accuracy[epoch] = test_acc\n",
    "        status = 'epoch: %d loss: %.5f train_err: %.3f train_acc: %.3f test_top1: %.3f test_loss %.5f test_acc: %.3f\\n' % (epoch, loss, train_err, train_acc, test_err, test_loss, test_acc)\n",
    "        print(status)\n",
    "       \n",
    "        if int(epoch) == 0.3*epochs or int(epoch) == 0.6*epochs or int(epoch) == 0.8*epochs:\n",
    "            lr_val *= lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= lr_decay\n",
    "\n",
    "    return train_losses, train_accuracy, val_losses, val_accuracy, net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ilV12w8TMwFh"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "start_epoch = 1\n",
    "lr_decay = 0.2\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "epochs = 200\n",
    "loss_name = 'crossentropy'\n",
    "\n",
    "# models = ['ResNet18', 'VGG16']\n",
    "# seeds = [0, 1, 2]\n",
    "# opts = ['sgd', 'adam']\n",
    "\n",
    "models = ['VGG16']\n",
    "seeds = [1]\n",
    "opts = ['adam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MtzHykmAXN5j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\write\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n",
      "  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.58940 train_err: 62.296 train_acc: 37.704 test_top1: 55.820 test_loss 1.45850 test_acc: 44.180\n",
      "\n",
      "epoch: 1 loss: 1.21454 train_err: 44.602 train_acc: 55.398 test_top1: 47.590 test_loss 1.40973 test_acc: 52.410\n",
      "\n",
      "epoch: 2 loss: 1.00555 train_err: 35.836 train_acc: 64.164 test_top1: 46.840 test_loss 1.35567 test_acc: 53.160\n",
      "\n",
      "epoch: 3 loss: 0.88283 train_err: 30.784 train_acc: 69.216 test_top1: 30.780 test_loss 0.90051 test_acc: 69.220\n",
      "\n",
      "epoch: 4 loss: 0.76695 train_err: 26.392 train_acc: 73.608 test_top1: 28.290 test_loss 0.83358 test_acc: 71.710\n",
      "\n",
      "epoch: 5 loss: 0.67615 train_err: 22.494 train_acc: 77.506 test_top1: 26.380 test_loss 0.79211 test_acc: 73.620\n",
      "\n",
      "epoch: 6 loss: 0.61453 train_err: 20.304 train_acc: 79.696 test_top1: 21.980 test_loss 0.68076 test_acc: 78.020\n",
      "\n",
      "epoch: 7 loss: 0.56609 train_err: 18.588 train_acc: 81.412 test_top1: 20.350 test_loss 0.62723 test_acc: 79.650\n",
      "\n",
      "epoch: 8 loss: 0.52387 train_err: 17.186 train_acc: 82.814 test_top1: 20.680 test_loss 0.63840 test_acc: 79.320\n",
      "\n",
      "epoch: 9 loss: 0.48855 train_err: 16.182 train_acc: 83.818 test_top1: 17.970 test_loss 0.55585 test_acc: 82.030\n",
      "\n",
      "epoch: 10 loss: 0.47319 train_err: 15.370 train_acc: 84.630 test_top1: 18.900 test_loss 0.60415 test_acc: 81.100\n",
      "\n",
      "epoch: 11 loss: 0.45583 train_err: 14.906 train_acc: 85.094 test_top1: 17.950 test_loss 0.55230 test_acc: 82.050\n",
      "\n",
      "epoch: 12 loss: 0.42427 train_err: 13.846 train_acc: 86.154 test_top1: 16.920 test_loss 0.51743 test_acc: 83.080\n",
      "\n",
      "epoch: 13 loss: 0.41427 train_err: 13.438 train_acc: 86.562 test_top1: 16.040 test_loss 0.49921 test_acc: 83.960\n",
      "\n",
      "epoch: 14 loss: 0.39786 train_err: 12.980 train_acc: 87.020 test_top1: 15.410 test_loss 0.47720 test_acc: 84.590\n",
      "\n",
      "epoch: 15 loss: 0.38988 train_err: 12.720 train_acc: 87.280 test_top1: 16.760 test_loss 0.52477 test_acc: 83.240\n",
      "\n",
      "epoch: 16 loss: 0.37839 train_err: 12.314 train_acc: 87.686 test_top1: 15.880 test_loss 0.50539 test_acc: 84.120\n",
      "\n",
      "epoch: 17 loss: 0.37079 train_err: 12.144 train_acc: 87.856 test_top1: 17.270 test_loss 0.54693 test_acc: 82.730\n",
      "\n",
      "epoch: 18 loss: 0.35815 train_err: 11.770 train_acc: 88.230 test_top1: 13.650 test_loss 0.41332 test_acc: 86.350\n",
      "\n",
      "epoch: 19 loss: 0.35212 train_err: 11.404 train_acc: 88.596 test_top1: 16.280 test_loss 0.50977 test_acc: 83.720\n",
      "\n",
      "epoch: 20 loss: 0.34912 train_err: 11.278 train_acc: 88.722 test_top1: 14.460 test_loss 0.45652 test_acc: 85.540\n",
      "\n",
      "epoch: 21 loss: 0.33289 train_err: 10.778 train_acc: 89.222 test_top1: 14.970 test_loss 0.46806 test_acc: 85.030\n",
      "\n",
      "epoch: 22 loss: 0.33456 train_err: 10.810 train_acc: 89.190 test_top1: 15.050 test_loss 0.46111 test_acc: 84.950\n",
      "\n",
      "epoch: 23 loss: 0.32766 train_err: 10.680 train_acc: 89.320 test_top1: 15.490 test_loss 0.48143 test_acc: 84.510\n",
      "\n",
      "epoch: 24 loss: 0.32090 train_err: 10.394 train_acc: 89.606 test_top1: 17.160 test_loss 0.55149 test_acc: 82.840\n",
      "\n",
      "epoch: 25 loss: 0.31649 train_err: 10.260 train_acc: 89.740 test_top1: 14.590 test_loss 0.46807 test_acc: 85.410\n",
      "\n",
      "epoch: 26 loss: 0.30654 train_err: 9.982 train_acc: 90.018 test_top1: 13.400 test_loss 0.42864 test_acc: 86.600\n",
      "\n",
      "epoch: 27 loss: 0.30373 train_err: 9.804 train_acc: 90.196 test_top1: 12.730 test_loss 0.40791 test_acc: 87.270\n",
      "\n",
      "epoch: 28 loss: 0.30671 train_err: 9.928 train_acc: 90.072 test_top1: 14.180 test_loss 0.44185 test_acc: 85.820\n",
      "\n",
      "epoch: 29 loss: 0.30130 train_err: 9.874 train_acc: 90.126 test_top1: 12.560 test_loss 0.40301 test_acc: 87.440\n",
      "\n",
      "epoch: 30 loss: 0.30115 train_err: 9.850 train_acc: 90.150 test_top1: 12.920 test_loss 0.42219 test_acc: 87.080\n",
      "\n",
      "epoch: 31 loss: 0.28896 train_err: 9.290 train_acc: 90.710 test_top1: 13.430 test_loss 0.43582 test_acc: 86.570\n",
      "\n",
      "epoch: 32 loss: 0.28892 train_err: 9.346 train_acc: 90.654 test_top1: 13.990 test_loss 0.43895 test_acc: 86.010\n",
      "\n",
      "epoch: 33 loss: 0.28362 train_err: 9.236 train_acc: 90.764 test_top1: 14.460 test_loss 0.46620 test_acc: 85.540\n",
      "\n",
      "epoch: 34 loss: 0.28680 train_err: 9.244 train_acc: 90.756 test_top1: 12.820 test_loss 0.40394 test_acc: 87.180\n",
      "\n",
      "epoch: 35 loss: 0.28001 train_err: 9.090 train_acc: 90.910 test_top1: 11.940 test_loss 0.38061 test_acc: 88.060\n",
      "\n",
      "epoch: 36 loss: 0.28269 train_err: 9.014 train_acc: 90.986 test_top1: 13.200 test_loss 0.42186 test_acc: 86.800\n",
      "\n",
      "epoch: 37 loss: 0.28124 train_err: 9.058 train_acc: 90.942 test_top1: 13.790 test_loss 0.42531 test_acc: 86.210\n",
      "\n",
      "epoch: 38 loss: 0.27405 train_err: 8.902 train_acc: 91.098 test_top1: 12.610 test_loss 0.41008 test_acc: 87.390\n",
      "\n",
      "epoch: 39 loss: 0.27757 train_err: 8.912 train_acc: 91.088 test_top1: 12.030 test_loss 0.37864 test_acc: 87.970\n",
      "\n",
      "epoch: 40 loss: 0.27444 train_err: 8.984 train_acc: 91.016 test_top1: 12.200 test_loss 0.39235 test_acc: 87.800\n",
      "\n",
      "epoch: 41 loss: 0.26680 train_err: 8.760 train_acc: 91.240 test_top1: 12.370 test_loss 0.39778 test_acc: 87.630\n",
      "\n",
      "epoch: 42 loss: 0.26828 train_err: 8.586 train_acc: 91.414 test_top1: 12.670 test_loss 0.40281 test_acc: 87.330\n",
      "\n",
      "epoch: 43 loss: 0.27202 train_err: 8.802 train_acc: 91.198 test_top1: 14.210 test_loss 0.46790 test_acc: 85.790\n",
      "\n",
      "epoch: 44 loss: 0.26787 train_err: 8.684 train_acc: 91.316 test_top1: 12.320 test_loss 0.39377 test_acc: 87.680\n",
      "\n",
      "epoch: 45 loss: 0.26755 train_err: 8.712 train_acc: 91.288 test_top1: 12.760 test_loss 0.41285 test_acc: 87.240\n",
      "\n",
      "epoch: 46 loss: 0.25999 train_err: 8.376 train_acc: 91.624 test_top1: 12.590 test_loss 0.39888 test_acc: 87.410\n",
      "\n",
      "epoch: 47 loss: 0.26023 train_err: 8.452 train_acc: 91.548 test_top1: 12.550 test_loss 0.39406 test_acc: 87.450\n",
      "\n",
      "epoch: 48 loss: 0.26361 train_err: 8.460 train_acc: 91.540 test_top1: 12.040 test_loss 0.38469 test_acc: 87.960\n",
      "\n",
      "epoch: 49 loss: 0.25875 train_err: 8.400 train_acc: 91.600 test_top1: 12.620 test_loss 0.39366 test_acc: 87.380\n",
      "\n",
      "epoch: 50 loss: 0.25853 train_err: 8.368 train_acc: 91.632 test_top1: 14.180 test_loss 0.44991 test_acc: 85.820\n",
      "\n",
      "epoch: 51 loss: 0.25505 train_err: 8.142 train_acc: 91.858 test_top1: 12.330 test_loss 0.38333 test_acc: 87.670\n",
      "\n",
      "epoch: 52 loss: 0.25537 train_err: 8.276 train_acc: 91.724 test_top1: 12.100 test_loss 0.39304 test_acc: 87.900\n",
      "\n",
      "epoch: 53 loss: 0.25390 train_err: 8.124 train_acc: 91.876 test_top1: 12.810 test_loss 0.42007 test_acc: 87.190\n",
      "\n",
      "epoch: 54 loss: 0.25384 train_err: 8.202 train_acc: 91.798 test_top1: 12.340 test_loss 0.40034 test_acc: 87.660\n",
      "\n",
      "epoch: 55 loss: 0.25188 train_err: 8.238 train_acc: 91.762 test_top1: 11.280 test_loss 0.36184 test_acc: 88.720\n",
      "\n",
      "epoch: 56 loss: 0.25464 train_err: 8.212 train_acc: 91.788 test_top1: 11.580 test_loss 0.37914 test_acc: 88.420\n",
      "\n",
      "epoch: 57 loss: 0.25284 train_err: 8.238 train_acc: 91.762 test_top1: 11.790 test_loss 0.37944 test_acc: 88.210\n",
      "\n",
      "epoch: 58 loss: 0.24864 train_err: 8.064 train_acc: 91.936 test_top1: 11.380 test_loss 0.37244 test_acc: 88.620\n",
      "\n",
      "epoch: 59 loss: 0.24781 train_err: 7.896 train_acc: 92.104 test_top1: 12.640 test_loss 0.40211 test_acc: 87.360\n",
      "\n",
      "epoch: 60 loss: 0.24827 train_err: 7.946 train_acc: 92.054 test_top1: 11.550 test_loss 0.37371 test_acc: 88.450\n",
      "\n",
      "epoch: 61 loss: 0.16182 train_err: 5.080 train_acc: 94.920 test_top1: 9.380 test_loss 0.30362 test_acc: 90.620\n",
      "\n",
      "epoch: 62 loss: 0.13179 train_err: 4.186 train_acc: 95.814 test_top1: 9.130 test_loss 0.29845 test_acc: 90.870\n",
      "\n",
      "epoch: 63 loss: 0.11977 train_err: 3.752 train_acc: 96.248 test_top1: 8.760 test_loss 0.30072 test_acc: 91.240\n",
      "\n",
      "epoch: 64 loss: 0.10917 train_err: 3.556 train_acc: 96.444 test_top1: 8.710 test_loss 0.30049 test_acc: 91.290\n",
      "\n",
      "epoch: 65 loss: 0.10708 train_err: 3.476 train_acc: 96.524 test_top1: 8.630 test_loss 0.29780 test_acc: 91.370\n",
      "\n",
      "epoch: 66 loss: 0.09809 train_err: 3.090 train_acc: 96.910 test_top1: 8.820 test_loss 0.30226 test_acc: 91.180\n",
      "\n",
      "epoch: 67 loss: 0.09353 train_err: 3.054 train_acc: 96.946 test_top1: 8.730 test_loss 0.30525 test_acc: 91.270\n",
      "\n",
      "epoch: 68 loss: 0.08790 train_err: 2.870 train_acc: 97.130 test_top1: 9.180 test_loss 0.32426 test_acc: 90.820\n",
      "\n",
      "epoch: 69 loss: 0.08714 train_err: 2.766 train_acc: 97.234 test_top1: 8.980 test_loss 0.31034 test_acc: 91.020\n",
      "\n",
      "epoch: 70 loss: 0.08212 train_err: 2.602 train_acc: 97.398 test_top1: 8.930 test_loss 0.31830 test_acc: 91.070\n",
      "\n",
      "epoch: 71 loss: 0.08195 train_err: 2.610 train_acc: 97.390 test_top1: 8.810 test_loss 0.32219 test_acc: 91.190\n",
      "\n",
      "epoch: 72 loss: 0.07602 train_err: 2.394 train_acc: 97.606 test_top1: 8.870 test_loss 0.32468 test_acc: 91.130\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 73 loss: 0.07738 train_err: 2.504 train_acc: 97.496 test_top1: 8.820 test_loss 0.32461 test_acc: 91.180\n",
      "\n",
      "epoch: 74 loss: 0.07359 train_err: 2.384 train_acc: 97.616 test_top1: 8.720 test_loss 0.31933 test_acc: 91.280\n",
      "\n",
      "epoch: 75 loss: 0.07304 train_err: 2.350 train_acc: 97.650 test_top1: 9.070 test_loss 0.32696 test_acc: 90.930\n",
      "\n",
      "epoch: 76 loss: 0.07286 train_err: 2.318 train_acc: 97.682 test_top1: 8.450 test_loss 0.31352 test_acc: 91.550\n",
      "\n",
      "epoch: 77 loss: 0.06942 train_err: 2.158 train_acc: 97.842 test_top1: 8.960 test_loss 0.32810 test_acc: 91.040\n",
      "\n",
      "epoch: 78 loss: 0.06917 train_err: 2.258 train_acc: 97.742 test_top1: 8.780 test_loss 0.32994 test_acc: 91.220\n",
      "\n",
      "epoch: 79 loss: 0.06689 train_err: 2.110 train_acc: 97.890 test_top1: 9.070 test_loss 0.33589 test_acc: 90.930\n",
      "\n",
      "epoch: 80 loss: 0.06419 train_err: 2.028 train_acc: 97.972 test_top1: 9.030 test_loss 0.34762 test_acc: 90.970\n",
      "\n",
      "epoch: 81 loss: 0.06354 train_err: 2.010 train_acc: 97.990 test_top1: 9.060 test_loss 0.34693 test_acc: 90.940\n",
      "\n",
      "epoch: 82 loss: 0.06323 train_err: 2.062 train_acc: 97.938 test_top1: 9.140 test_loss 0.34575 test_acc: 90.860\n",
      "\n",
      "epoch: 83 loss: 0.06394 train_err: 2.080 train_acc: 97.920 test_top1: 9.210 test_loss 0.35025 test_acc: 90.790\n",
      "\n",
      "epoch: 84 loss: 0.06360 train_err: 2.038 train_acc: 97.962 test_top1: 8.950 test_loss 0.34353 test_acc: 91.050\n",
      "\n",
      "epoch: 85 loss: 0.06383 train_err: 2.022 train_acc: 97.978 test_top1: 8.930 test_loss 0.35048 test_acc: 91.070\n",
      "\n",
      "epoch: 86 loss: 0.06055 train_err: 1.936 train_acc: 98.064 test_top1: 9.130 test_loss 0.34427 test_acc: 90.870\n",
      "\n",
      "epoch: 87 loss: 0.06170 train_err: 1.936 train_acc: 98.064 test_top1: 8.820 test_loss 0.34866 test_acc: 91.180\n",
      "\n",
      "epoch: 88 loss: 0.06016 train_err: 1.900 train_acc: 98.100 test_top1: 9.320 test_loss 0.36349 test_acc: 90.680\n",
      "\n",
      "epoch: 89 loss: 0.05791 train_err: 1.910 train_acc: 98.090 test_top1: 9.030 test_loss 0.35763 test_acc: 90.970\n",
      "\n",
      "epoch: 90 loss: 0.05875 train_err: 1.808 train_acc: 98.192 test_top1: 8.870 test_loss 0.34795 test_acc: 91.130\n",
      "\n",
      "epoch: 91 loss: 0.05835 train_err: 1.852 train_acc: 98.148 test_top1: 8.720 test_loss 0.34560 test_acc: 91.280\n",
      "\n",
      "epoch: 92 loss: 0.05582 train_err: 1.736 train_acc: 98.264 test_top1: 8.770 test_loss 0.35459 test_acc: 91.230\n",
      "\n",
      "epoch: 93 loss: 0.06189 train_err: 1.936 train_acc: 98.064 test_top1: 9.320 test_loss 0.35409 test_acc: 90.680\n",
      "\n",
      "epoch: 94 loss: 0.06119 train_err: 1.902 train_acc: 98.098 test_top1: 8.980 test_loss 0.34622 test_acc: 91.020\n",
      "\n",
      "epoch: 95 loss: 0.05564 train_err: 1.774 train_acc: 98.226 test_top1: 8.760 test_loss 0.35021 test_acc: 91.240\n",
      "\n",
      "epoch: 96 loss: 0.05514 train_err: 1.670 train_acc: 98.330 test_top1: 9.260 test_loss 0.36734 test_acc: 90.740\n",
      "\n",
      "epoch: 97 loss: 0.05453 train_err: 1.732 train_acc: 98.268 test_top1: 9.340 test_loss 0.38016 test_acc: 90.660\n",
      "\n",
      "epoch: 98 loss: 0.05792 train_err: 1.800 train_acc: 98.200 test_top1: 9.020 test_loss 0.36148 test_acc: 90.980\n",
      "\n",
      "epoch: 99 loss: 0.05679 train_err: 1.788 train_acc: 98.212 test_top1: 9.130 test_loss 0.36158 test_acc: 90.870\n",
      "\n",
      "epoch: 100 loss: 0.05538 train_err: 1.806 train_acc: 98.194 test_top1: 9.010 test_loss 0.36464 test_acc: 90.990\n",
      "\n",
      "epoch: 101 loss: 0.05744 train_err: 1.736 train_acc: 98.264 test_top1: 9.090 test_loss 0.36288 test_acc: 90.910\n",
      "\n",
      "epoch: 102 loss: 0.05420 train_err: 1.724 train_acc: 98.276 test_top1: 9.020 test_loss 0.36184 test_acc: 90.980\n",
      "\n",
      "epoch: 103 loss: 0.05638 train_err: 1.804 train_acc: 98.196 test_top1: 8.910 test_loss 0.34940 test_acc: 91.090\n",
      "\n",
      "epoch: 104 loss: 0.05243 train_err: 1.630 train_acc: 98.370 test_top1: 8.840 test_loss 0.35631 test_acc: 91.160\n",
      "\n",
      "epoch: 105 loss: 0.05282 train_err: 1.584 train_acc: 98.416 test_top1: 9.140 test_loss 0.36053 test_acc: 90.860\n",
      "\n",
      "epoch: 106 loss: 0.05578 train_err: 1.790 train_acc: 98.210 test_top1: 9.330 test_loss 0.37096 test_acc: 90.670\n",
      "\n",
      "epoch: 107 loss: 0.05406 train_err: 1.646 train_acc: 98.354 test_top1: 9.290 test_loss 0.37575 test_acc: 90.710\n",
      "\n",
      "epoch: 108 loss: 0.05741 train_err: 1.756 train_acc: 98.244 test_top1: 9.230 test_loss 0.37047 test_acc: 90.770\n",
      "\n",
      "epoch: 109 loss: 0.05405 train_err: 1.758 train_acc: 98.242 test_top1: 8.780 test_loss 0.35576 test_acc: 91.220\n",
      "\n",
      "epoch: 110 loss: 0.05373 train_err: 1.666 train_acc: 98.334 test_top1: 8.970 test_loss 0.35696 test_acc: 91.030\n",
      "\n",
      "epoch: 111 loss: 0.05084 train_err: 1.552 train_acc: 98.448 test_top1: 9.020 test_loss 0.36083 test_acc: 90.980\n",
      "\n",
      "epoch: 112 loss: 0.05263 train_err: 1.572 train_acc: 98.428 test_top1: 9.170 test_loss 0.35934 test_acc: 90.830\n",
      "\n",
      "epoch: 113 loss: 0.05339 train_err: 1.720 train_acc: 98.280 test_top1: 8.880 test_loss 0.37166 test_acc: 91.120\n",
      "\n",
      "epoch: 114 loss: 0.05380 train_err: 1.628 train_acc: 98.372 test_top1: 9.100 test_loss 0.36954 test_acc: 90.900\n",
      "\n",
      "epoch: 115 loss: 0.05322 train_err: 1.608 train_acc: 98.392 test_top1: 8.960 test_loss 0.36341 test_acc: 91.040\n",
      "\n",
      "epoch: 116 loss: 0.05257 train_err: 1.638 train_acc: 98.362 test_top1: 9.020 test_loss 0.36611 test_acc: 90.980\n",
      "\n",
      "epoch: 117 loss: 0.05371 train_err: 1.668 train_acc: 98.332 test_top1: 9.150 test_loss 0.36929 test_acc: 90.850\n",
      "\n",
      "epoch: 118 loss: 0.05274 train_err: 1.670 train_acc: 98.330 test_top1: 9.350 test_loss 0.38697 test_acc: 90.650\n",
      "\n",
      "epoch: 119 loss: 0.05141 train_err: 1.626 train_acc: 98.374 test_top1: 9.320 test_loss 0.39109 test_acc: 90.680\n",
      "\n",
      "epoch: 120 loss: 0.05496 train_err: 1.756 train_acc: 98.244 test_top1: 9.220 test_loss 0.37418 test_acc: 90.780\n",
      "\n",
      "epoch: 121 loss: 0.03680 train_err: 1.098 train_acc: 98.902 test_top1: 8.710 test_loss 0.35536 test_acc: 91.290\n",
      "\n",
      "epoch: 122 loss: 0.02798 train_err: 0.802 train_acc: 99.198 test_top1: 8.470 test_loss 0.34913 test_acc: 91.530\n",
      "\n",
      "epoch: 123 loss: 0.02504 train_err: 0.772 train_acc: 99.228 test_top1: 8.520 test_loss 0.35750 test_acc: 91.480\n",
      "\n",
      "epoch: 124 loss: 0.02343 train_err: 0.644 train_acc: 99.356 test_top1: 8.480 test_loss 0.35885 test_acc: 91.520\n",
      "\n",
      "epoch: 125 loss: 0.02212 train_err: 0.638 train_acc: 99.362 test_top1: 8.380 test_loss 0.35707 test_acc: 91.620\n",
      "\n",
      "epoch: 126 loss: 0.02221 train_err: 0.654 train_acc: 99.346 test_top1: 8.410 test_loss 0.36025 test_acc: 91.590\n",
      "\n",
      "epoch: 127 loss: 0.02083 train_err: 0.580 train_acc: 99.420 test_top1: 8.330 test_loss 0.36422 test_acc: 91.670\n",
      "\n",
      "epoch: 128 loss: 0.02099 train_err: 0.604 train_acc: 99.396 test_top1: 8.120 test_loss 0.36389 test_acc: 91.880\n",
      "\n",
      "epoch: 129 loss: 0.01813 train_err: 0.504 train_acc: 99.496 test_top1: 8.180 test_loss 0.36486 test_acc: 91.820\n",
      "\n",
      "epoch: 130 loss: 0.01817 train_err: 0.526 train_acc: 99.474 test_top1: 8.080 test_loss 0.36259 test_acc: 91.920\n",
      "\n",
      "epoch: 131 loss: 0.01801 train_err: 0.476 train_acc: 99.524 test_top1: 8.090 test_loss 0.35889 test_acc: 91.910\n",
      "\n",
      "epoch: 132 loss: 0.01658 train_err: 0.438 train_acc: 99.562 test_top1: 8.060 test_loss 0.36959 test_acc: 91.940\n",
      "\n",
      "epoch: 133 loss: 0.01585 train_err: 0.442 train_acc: 99.558 test_top1: 8.120 test_loss 0.36513 test_acc: 91.880\n",
      "\n",
      "epoch: 134 loss: 0.01632 train_err: 0.436 train_acc: 99.564 test_top1: 8.170 test_loss 0.36910 test_acc: 91.830\n",
      "\n",
      "epoch: 135 loss: 0.01720 train_err: 0.480 train_acc: 99.520 test_top1: 8.140 test_loss 0.36584 test_acc: 91.860\n",
      "\n",
      "epoch: 136 loss: 0.01624 train_err: 0.462 train_acc: 99.538 test_top1: 8.240 test_loss 0.37448 test_acc: 91.760\n",
      "\n",
      "epoch: 137 loss: 0.01503 train_err: 0.412 train_acc: 99.588 test_top1: 8.040 test_loss 0.37669 test_acc: 91.960\n",
      "\n",
      "epoch: 138 loss: 0.01459 train_err: 0.374 train_acc: 99.626 test_top1: 8.260 test_loss 0.37310 test_acc: 91.740\n",
      "\n",
      "epoch: 139 loss: 0.01452 train_err: 0.384 train_acc: 99.616 test_top1: 8.230 test_loss 0.37981 test_acc: 91.770\n",
      "\n",
      "epoch: 140 loss: 0.01607 train_err: 0.426 train_acc: 99.574 test_top1: 8.440 test_loss 0.38035 test_acc: 91.560\n",
      "\n",
      "epoch: 141 loss: 0.01431 train_err: 0.394 train_acc: 99.606 test_top1: 8.270 test_loss 0.38455 test_acc: 91.730\n",
      "\n",
      "epoch: 142 loss: 0.01275 train_err: 0.352 train_acc: 99.648 test_top1: 8.350 test_loss 0.38396 test_acc: 91.650\n",
      "\n",
      "epoch: 143 loss: 0.01488 train_err: 0.396 train_acc: 99.604 test_top1: 8.250 test_loss 0.38338 test_acc: 91.750\n",
      "\n",
      "epoch: 144 loss: 0.01344 train_err: 0.320 train_acc: 99.680 test_top1: 8.330 test_loss 0.38321 test_acc: 91.670\n",
      "\n",
      "epoch: 145 loss: 0.01404 train_err: 0.366 train_acc: 99.634 test_top1: 8.240 test_loss 0.38285 test_acc: 91.760\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 146 loss: 0.01351 train_err: 0.344 train_acc: 99.656 test_top1: 8.150 test_loss 0.38074 test_acc: 91.850\n",
      "\n",
      "epoch: 147 loss: 0.01436 train_err: 0.368 train_acc: 99.632 test_top1: 8.130 test_loss 0.38085 test_acc: 91.870\n",
      "\n",
      "epoch: 148 loss: 0.01444 train_err: 0.400 train_acc: 99.600 test_top1: 8.100 test_loss 0.38235 test_acc: 91.900\n",
      "\n",
      "epoch: 149 loss: 0.01354 train_err: 0.348 train_acc: 99.652 test_top1: 8.230 test_loss 0.38645 test_acc: 91.770\n",
      "\n",
      "epoch: 150 loss: 0.01257 train_err: 0.338 train_acc: 99.662 test_top1: 8.450 test_loss 0.38946 test_acc: 91.550\n",
      "\n",
      "epoch: 151 loss: 0.01328 train_err: 0.334 train_acc: 99.666 test_top1: 8.360 test_loss 0.39140 test_acc: 91.640\n",
      "\n",
      "epoch: 152 loss: 0.01228 train_err: 0.328 train_acc: 99.672 test_top1: 8.070 test_loss 0.38801 test_acc: 91.930\n",
      "\n",
      "epoch: 153 loss: 0.01326 train_err: 0.356 train_acc: 99.644 test_top1: 8.340 test_loss 0.39772 test_acc: 91.660\n",
      "\n",
      "epoch: 154 loss: 0.01234 train_err: 0.328 train_acc: 99.672 test_top1: 8.340 test_loss 0.39157 test_acc: 91.660\n",
      "\n",
      "epoch: 155 loss: 0.01165 train_err: 0.290 train_acc: 99.710 test_top1: 8.160 test_loss 0.39174 test_acc: 91.840\n",
      "\n",
      "epoch: 156 loss: 0.01187 train_err: 0.328 train_acc: 99.672 test_top1: 8.210 test_loss 0.39232 test_acc: 91.790\n",
      "\n",
      "epoch: 157 loss: 0.01341 train_err: 0.354 train_acc: 99.646 test_top1: 8.330 test_loss 0.39502 test_acc: 91.670\n",
      "\n",
      "epoch: 158 loss: 0.01266 train_err: 0.364 train_acc: 99.636 test_top1: 8.360 test_loss 0.40611 test_acc: 91.640\n",
      "\n",
      "epoch: 159 loss: 0.01281 train_err: 0.328 train_acc: 99.672 test_top1: 8.160 test_loss 0.39934 test_acc: 91.840\n",
      "\n",
      "epoch: 160 loss: 0.01154 train_err: 0.312 train_acc: 99.688 test_top1: 8.130 test_loss 0.39768 test_acc: 91.870\n",
      "\n",
      "epoch: 161 loss: 0.01047 train_err: 0.236 train_acc: 99.764 test_top1: 8.060 test_loss 0.39783 test_acc: 91.940\n",
      "\n",
      "epoch: 162 loss: 0.00988 train_err: 0.252 train_acc: 99.748 test_top1: 8.290 test_loss 0.39534 test_acc: 91.710\n",
      "\n",
      "epoch: 163 loss: 0.00939 train_err: 0.216 train_acc: 99.784 test_top1: 8.090 test_loss 0.39645 test_acc: 91.910\n",
      "\n",
      "epoch: 164 loss: 0.00922 train_err: 0.230 train_acc: 99.770 test_top1: 8.130 test_loss 0.39488 test_acc: 91.870\n",
      "\n",
      "epoch: 165 loss: 0.00977 train_err: 0.250 train_acc: 99.750 test_top1: 7.970 test_loss 0.38966 test_acc: 92.030\n",
      "\n",
      "epoch: 166 loss: 0.00837 train_err: 0.206 train_acc: 99.794 test_top1: 7.990 test_loss 0.39131 test_acc: 92.010\n",
      "\n",
      "epoch: 167 loss: 0.00891 train_err: 0.190 train_acc: 99.810 test_top1: 7.900 test_loss 0.38859 test_acc: 92.100\n",
      "\n",
      "epoch: 168 loss: 0.00898 train_err: 0.204 train_acc: 99.796 test_top1: 7.940 test_loss 0.39369 test_acc: 92.060\n",
      "\n",
      "epoch: 169 loss: 0.00809 train_err: 0.186 train_acc: 99.814 test_top1: 7.930 test_loss 0.39474 test_acc: 92.070\n",
      "\n",
      "epoch: 170 loss: 0.00818 train_err: 0.184 train_acc: 99.816 test_top1: 7.880 test_loss 0.39477 test_acc: 92.120\n",
      "\n",
      "epoch: 171 loss: 0.00796 train_err: 0.166 train_acc: 99.834 test_top1: 8.030 test_loss 0.39425 test_acc: 91.970\n",
      "\n",
      "epoch: 172 loss: 0.00801 train_err: 0.172 train_acc: 99.828 test_top1: 8.080 test_loss 0.39487 test_acc: 91.920\n",
      "\n",
      "epoch: 173 loss: 0.00805 train_err: 0.184 train_acc: 99.816 test_top1: 7.830 test_loss 0.39525 test_acc: 92.170\n",
      "\n",
      "epoch: 174 loss: 0.00764 train_err: 0.158 train_acc: 99.842 test_top1: 7.930 test_loss 0.39436 test_acc: 92.070\n",
      "\n",
      "epoch: 175 loss: 0.00847 train_err: 0.176 train_acc: 99.824 test_top1: 7.940 test_loss 0.39228 test_acc: 92.060\n",
      "\n",
      "epoch: 176 loss: 0.00768 train_err: 0.162 train_acc: 99.838 test_top1: 7.970 test_loss 0.39446 test_acc: 92.030\n",
      "\n",
      "epoch: 177 loss: 0.00753 train_err: 0.158 train_acc: 99.842 test_top1: 7.890 test_loss 0.39152 test_acc: 92.110\n",
      "\n",
      "epoch: 178 loss: 0.00800 train_err: 0.192 train_acc: 99.808 test_top1: 7.960 test_loss 0.39378 test_acc: 92.040\n",
      "\n",
      "epoch: 179 loss: 0.00725 train_err: 0.156 train_acc: 99.844 test_top1: 8.110 test_loss 0.39615 test_acc: 91.890\n",
      "\n",
      "epoch: 180 loss: 0.00792 train_err: 0.180 train_acc: 99.820 test_top1: 7.920 test_loss 0.39288 test_acc: 92.080\n",
      "\n",
      "epoch: 181 loss: 0.00812 train_err: 0.170 train_acc: 99.830 test_top1: 7.940 test_loss 0.39481 test_acc: 92.060\n",
      "\n",
      "epoch: 182 loss: 0.00765 train_err: 0.178 train_acc: 99.822 test_top1: 7.980 test_loss 0.39891 test_acc: 92.020\n",
      "\n",
      "epoch: 183 loss: 0.00743 train_err: 0.186 train_acc: 99.814 test_top1: 7.840 test_loss 0.39596 test_acc: 92.160\n",
      "\n",
      "epoch: 184 loss: 0.00708 train_err: 0.158 train_acc: 99.842 test_top1: 7.860 test_loss 0.39615 test_acc: 92.140\n",
      "\n",
      "epoch: 185 loss: 0.00720 train_err: 0.142 train_acc: 99.858 test_top1: 7.910 test_loss 0.39791 test_acc: 92.090\n",
      "\n",
      "epoch: 186 loss: 0.00688 train_err: 0.146 train_acc: 99.854 test_top1: 7.980 test_loss 0.39726 test_acc: 92.020\n",
      "\n",
      "epoch: 187 loss: 0.00718 train_err: 0.158 train_acc: 99.842 test_top1: 8.020 test_loss 0.39855 test_acc: 91.980\n",
      "\n",
      "epoch: 188 loss: 0.00694 train_err: 0.126 train_acc: 99.874 test_top1: 7.980 test_loss 0.40009 test_acc: 92.020\n",
      "\n",
      "epoch: 189 loss: 0.00732 train_err: 0.148 train_acc: 99.852 test_top1: 7.920 test_loss 0.40096 test_acc: 92.080\n",
      "\n",
      "epoch: 190 loss: 0.00682 train_err: 0.136 train_acc: 99.864 test_top1: 7.880 test_loss 0.40102 test_acc: 92.120\n",
      "\n",
      "epoch: 191 loss: 0.00702 train_err: 0.136 train_acc: 99.864 test_top1: 7.960 test_loss 0.40091 test_acc: 92.040\n",
      "\n",
      "epoch: 192 loss: 0.00818 train_err: 0.210 train_acc: 99.790 test_top1: 7.940 test_loss 0.40459 test_acc: 92.060\n",
      "\n",
      "epoch: 193 loss: 0.00647 train_err: 0.124 train_acc: 99.876 test_top1: 8.030 test_loss 0.40326 test_acc: 91.970\n",
      "\n",
      "epoch: 194 loss: 0.00740 train_err: 0.164 train_acc: 99.836 test_top1: 7.940 test_loss 0.39711 test_acc: 92.060\n",
      "\n",
      "epoch: 195 loss: 0.00697 train_err: 0.134 train_acc: 99.866 test_top1: 7.940 test_loss 0.40026 test_acc: 92.060\n",
      "\n",
      "epoch: 196 loss: 0.00731 train_err: 0.128 train_acc: 99.872 test_top1: 7.920 test_loss 0.39954 test_acc: 92.080\n",
      "\n",
      "epoch: 197 loss: 0.00722 train_err: 0.152 train_acc: 99.848 test_top1: 7.920 test_loss 0.40488 test_acc: 92.080\n",
      "\n",
      "epoch: 198 loss: 0.00723 train_err: 0.166 train_acc: 99.834 test_top1: 8.060 test_loss 0.40303 test_acc: 91.940\n",
      "\n",
      "epoch: 199 loss: 0.00681 train_err: 0.144 train_acc: 99.856 test_top1: 8.060 test_loss 0.40411 test_acc: 91.940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    for rand_seed in seeds:\n",
    "        for optimizer_val in opts:\n",
    "            train_losses, train_accuracy, val_losses, val_accuracy, net = main(model_name, rand_seed, optimizer_val)\n",
    "\n",
    "            torch.save(net.state_dict(), f'{optimizer_val}_baseline_{model_name}_seed{rand_seed}.pt')\n",
    "            with open(f'{optimizer_val}_baseline_{model_name}_seed{rand_seed}.npy', 'wb') as numpy_file:\n",
    "                np.save(numpy_file, train_losses)\n",
    "                np.save(numpy_file, train_accuracy)\n",
    "                np.save(numpy_file, val_losses)\n",
    "                np.save(numpy_file, val_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baseline_2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
