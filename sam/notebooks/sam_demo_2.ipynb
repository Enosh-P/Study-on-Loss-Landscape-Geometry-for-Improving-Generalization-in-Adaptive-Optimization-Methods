{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1dc2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enosh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\enosh\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "from utility.log import Log\n",
    "from model.vgg import VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e17c1",
   "metadata": {},
   "source": [
    "### Smooth entropy done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510467ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_crossentropy(pred, gold, smoothing=0.1):\n",
    "    n_class = pred.size(1)\n",
    "\n",
    "    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n",
    "    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n",
    "    log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0400a",
   "metadata": {},
   "source": [
    "### Data Preparation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b9926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar:\n",
    "    def __init__(self, batch_size, threads):\n",
    "        mean, std = self._get_statistics()\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            torchvision.transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "        self.train = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=threads)\n",
    "        self.test = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=threads)\n",
    "\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    def _get_statistics(self):\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./cifar', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "        data = torch.cat([d[0] for d in DataLoader(train_set)])\n",
    "        return data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717adc",
   "metadata": {},
   "source": [
    "### Step LR and Initialize and Bypass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d32d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR:\n",
    "    def __init__(self, optimizer, learning_rate: float, total_epochs: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base = learning_rate\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        if epoch < self.total_epochs * 3/10:\n",
    "            lr = self.base\n",
    "        elif epoch < self.total_epochs * 6/10:\n",
    "            lr = self.base * 0.2\n",
    "        elif epoch < self.total_epochs * 8/10:\n",
    "            lr = self.base * 0.2 ** 2\n",
    "        else:\n",
    "            lr = self.base * 0.2 ** 3\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def lr(self) -> float:\n",
    "        return self.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce91b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c65cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0623d5",
   "metadata": {},
   "source": [
    "### SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcba7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158db4bf",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4a56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "adaptive = True\n",
    "batch_size = 32\n",
    "depth = 16\n",
    "dropout = 0.0\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "threads = 2\n",
    "rho = 2.0\n",
    "weight_decay = 0.0005\n",
    "width_factor = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968a5cf",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feedca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "initialize(seed=42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01cee922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar(batch_size, threads)\n",
    "log = Log(log_each=10)\n",
    "# model = WideResNet(depth, width_factor, dropout, in_channels=3, labels=10).to(device)\n",
    "model = VGG('VGG16').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd06c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.Adam\n",
    "# base_optimizer = torch.optim.SGD\n",
    "\n",
    "optimizer = SAM(\n",
    "    model.parameters(), \n",
    "    base_optimizer, \n",
    "    rho=rho, \n",
    "    adaptive=adaptive, \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288d9a3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
      "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
      "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
      "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    log.train(len_dataset=len(dataset.train))\n",
    "    \n",
    "    for batch in dataset.train:\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)\n",
    "        smooth_crossentropy(model(inputs), targets, smoothing=label_smoothing).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu())\n",
    "log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31758a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cacce2d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
      "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
      "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
      "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n",
      "┃           0  ┃      1.9893  │     12.52 %  ┃   1.000e-03  │   00:48 min  ┃      2.2263  │     18.26 %  ┃\n",
      "┃           1  ┃      1.6752  │     16.75 %  ┃   1.000e-03  │   00:52 min  ┃      1.6856  │     20.65 %  ┃\n",
      "┃           2  ┃      1.6351  │     18.52 %  ┃   1.000e-03  │   00:58 min  ┃      1.6210  │     18.77 %  ┃\n",
      "┃           3  ┃      1.6303  │     17.93 %  ┃   1.000e-03  │   01:02 min  ┃      1.5918  │     18.63 %  ┃\n",
      "┃           4  ┃      1.6179  │     19.56 %  ┃   1.000e-03  │   01:04 min  ┃      1.5887  │     21.98 %  ┃\n",
      "┃           5  ┃      1.6132  │     19.43 %  ┃   1.000e-03  │   01:07 min  ┃      1.5898  │     17.97 %  ┃\n",
      "┃           6  ┃      1.6096  │     19.17 %  ┃   1.000e-03  │   01:13 min  ┃      1.5842  │     21.69 %  ┃\n",
      "┃           7  ┃      1.6093  │     19.75 %  ┃   1.000e-03  │   01:17 min  ┃      1.5637  │     21.41 %  ┃\n",
      "┃           8  ┃      1.6021  │     19.53 %  ┃   1.000e-03  │   01:28 min  ┃      1.6169  │     21.45 %  ┃\n",
      "┃           9  ┃      1.5999  │     19.25 %  ┃   1.000e-03  │   01:28 min  ┃      1.5641  │     20.58 %  ┃\n",
      "┃          10  ┃      1.5858  │     19.79 %  ┃   1.000e-03  │   01:34 min  ┃      1.5639  │     22.28 %  ┃\n",
      "┃          11  ┃      1.5712  │     21.31 %  ┃   1.000e-03  │   01:48 min  ┃      2.2252  │     23.34 %  ┃\n",
      "┃          12  ┃      1.5499  │     20.70 %  ┃   1.000e-03  │   02:04 min  ┃      3.9527  │     20.90 %  ┃\n",
      "┃          13  ┃      1.5266  │     22.04 %  ┃   1.000e-03  │   02:19 min  ┃      4.3001  │     25.16 %  ┃\n",
      "┃          14  ┃      1.5090  │     23.31 %  ┃   1.000e-03  │   02:36 min  ┃      1.5642  │     24.81 %  ┃\n",
      "┃          15  ┃      1.4944  │     23.71 %  ┃   1.000e-03  │   02:53 min  ┃      1.4807  │     27.90 %  ┃\n",
      "┃          16  ┃      1.4923  │     23.35 %  ┃   1.000e-03  │   03:05 min  ┃      1.4692  │     23.14 %  ┃\n",
      "┃          17  ┃      1.4848  │     23.42 %  ┃   1.000e-03  │   03:26 min  ┃      1.4232  │     25.17 %  ┃\n",
      "┃          18  ┃      1.4805  │     23.93 %  ┃   1.000e-03  │   03:33 min  ┃      1.4325  │     24.36 %  ┃\n",
      "┃          19  ┃      1.4803  │     22.85 %  ┃   1.000e-03  │   03:46 min  ┃      1.4141  │     27.61 %  ┃\n",
      "┃          20  ┃      1.4608  │     23.33 %  ┃   1.000e-03  │   03:49 min  ┃      1.4168  │     26.84 %  ┃\n",
      "┃          21  ┃      1.4471  │     24.13 %  ┃   1.000e-03  │   03:51 min  ┃      1.4025  │     26.51 %  ┃\n",
      "┃          22  ┃      1.4419  │     24.82 %  ┃   1.000e-03  │   03:56 min  ┃      1.4263  │     25.76 %  ┃\n",
      "┃          23  ┃      1.4281  │     25.47 %  ┃   1.000e-03  │   04:20 min  ┃      1.3866  │     28.15 %  ┃\n",
      "┃          24  ┃      1.4235  │     25.53 %  ┃   1.000e-03  │   04:20 min  ┃      1.3781  │     27.24 %  ┃\n",
      "┃          25  ┃      1.4155  │     25.79 %  ┃   1.000e-03  │   04:07 min  ┃      1.4167  │     27.03 %  ┃\n",
      "┃          26  ┃      1.4077  │     25.74 %  ┃   1.000e-03  │   04:23 min  ┃      1.3588  │     27.65 %  ┃\n",
      "┃          27  ┃      1.4512  │     25.00 %  ┃   1.000e-03  │   03:43 min  ┠┈██████████████████████████░┈┨"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sohom/miniconda3/envs/nnti-project/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/sohom/miniconda3/envs/nnti-project/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/sohom/miniconda3/envs/nnti-project/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/sohom/miniconda3/envs/nnti-project/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-19-5371903f3c4e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m             \u001B[0mcorrect\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mtargets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m             \u001B[0mlog\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcorrect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mscheduler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m             \u001B[0mscheduler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    log.train(len_dataset=len(dataset.train))\n",
    "    \n",
    "    for batch in dataset.train:\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu())\n",
    "log.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}