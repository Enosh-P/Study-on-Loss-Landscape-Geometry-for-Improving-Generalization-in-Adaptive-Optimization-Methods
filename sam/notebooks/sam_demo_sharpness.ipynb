{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a1dc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "from utility.log import Log\n",
    "from model.vgg import VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e17c1",
   "metadata": {},
   "source": [
    "### Smooth entropy done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510467ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_crossentropy(pred, gold, smoothing=0.1):\n",
    "    n_class = pred.size(1)\n",
    "\n",
    "    one_hot = torch.full_like(pred, fill_value=smoothing / (n_class - 1))\n",
    "    one_hot.scatter_(dim=1, index=gold.unsqueeze(1), value=1.0 - smoothing)\n",
    "    log_prob = F.log_softmax(pred, dim=1)\n",
    "\n",
    "    return F.kl_div(input=log_prob, target=one_hot, reduction='none').sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e0400a",
   "metadata": {},
   "source": [
    "### Data Preparation done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca0ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout:\n",
    "    def __init__(self, size=16, p=0.5):\n",
    "        self.size = size\n",
    "        self.half_size = size // 2\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if torch.rand([1]).item() > self.p:\n",
    "            return image\n",
    "\n",
    "        left = torch.randint(-self.half_size, image.size(1) - self.half_size, [1]).item()\n",
    "        top = torch.randint(-self.half_size, image.size(2) - self.half_size, [1]).item()\n",
    "        right = min(image.size(1), left + self.size)\n",
    "        bottom = min(image.size(2), top + self.size)\n",
    "\n",
    "        image[:, max(0, left): right, max(0, top): bottom] = 0\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b9926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar:\n",
    "    def __init__(self, batch_size, threads):\n",
    "        mean, std = self._get_statistics()\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            torchvision.transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std),\n",
    "            Cutout()\n",
    "        ])\n",
    "\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "        self.train = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=threads)\n",
    "        self.test = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=threads)\n",
    "\n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    def _get_statistics(self):\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./cifar', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "        data = torch.cat([d[0] for d in DataLoader(train_set)])\n",
    "        return data.mean(dim=[0, 2, 3]), data.std(dim=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717adc",
   "metadata": {},
   "source": [
    "### Step LR and Initialize and Bypass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d32d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLR:\n",
    "    def __init__(self, optimizer, learning_rate: float, total_epochs: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.total_epochs = total_epochs\n",
    "        self.base = learning_rate\n",
    "\n",
    "    def __call__(self, epoch):\n",
    "        if epoch < self.total_epochs * 3/10:\n",
    "            lr = self.base\n",
    "        elif epoch < self.total_epochs * 6/10:\n",
    "            lr = self.base * 0.2\n",
    "        elif epoch < self.total_epochs * 8/10:\n",
    "            lr = self.base * 0.2 ** 2\n",
    "        else:\n",
    "            lr = self.base * 0.2 ** 3\n",
    "\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "    def lr(self) -> float:\n",
    "        return self.optimizer.param_groups[0][\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce91b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c65cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disable_running_stats(model):\n",
    "    def _disable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.backup_momentum = module.momentum\n",
    "            module.momentum = 0\n",
    "\n",
    "    model.apply(_disable)\n",
    "\n",
    "def enable_running_stats(model):\n",
    "    def _enable(module):\n",
    "        if isinstance(module, nn.BatchNorm2d) and hasattr(module, \"backup_momentum\"):\n",
    "            module.momentum = module.backup_momentum\n",
    "\n",
    "    model.apply(_enable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0623d5",
   "metadata": {},
   "source": [
    "### SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcba7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.sharpness = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "            self.sharpness = group[\"rho\"] * (grad_norm)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                self.state[p][\"old_p\"] = p.data.clone()\n",
    "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                # p.add_(0)\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        super().load_state_dict(state_dict)\n",
    "        self.base_optimizer.param_groups = self.param_groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158db4bf",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a4a56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "adaptive = True\n",
    "batch_size = 32\n",
    "depth = 16\n",
    "dropout = 0.0\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "threads = 2\n",
    "rho = 2.0\n",
    "weight_decay = 0.0005\n",
    "width_factor = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968a5cf",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feedca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "initialize(seed=42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01cee922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar(batch_size, threads)\n",
    "log = Log(log_each=10)\n",
    "# model = WideResNet(depth, width_factor, dropout, in_channels=3, labels=10).to(device)\n",
    "model = VGG('VGG16').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd06c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.Adam\n",
    "# base_optimizer = torch.optim.SGD\n",
    "\n",
    "optimizer = SAM(\n",
    "    model.parameters(), \n",
    "    base_optimizer, \n",
    "    rho=rho, \n",
    "    adaptive=adaptive, \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7288d9a3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    log.train(len_dataset=len(dataset.train))\n",
    "    \n",
    "    sharpness_accum = 0\n",
    "    for idx, batch in enumerate(dataset.train):\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        sharpness_accum += optimizer.sharpness\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)\n",
    "        smooth_crossentropy(model(inputs), targets, smoothing=label_smoothing).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), sharpness_accum/idx)\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu())\n",
    "log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31758a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(\n",
    "#     model.parameters(), \n",
    "#     lr=learning_rate, \n",
    "#     weight_decay=weight_decay\n",
    "# )\n",
    "# scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacce2d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     log.train(len_dataset=len(dataset.train))\n",
    "    \n",
    "#     for batch in dataset.train:\n",
    "#         inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "#         enable_running_stats(model)\n",
    "#         predictions = model(inputs)\n",
    "#         loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "#         loss.mean().backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             correct = torch.argmax(predictions.data, 1) == targets\n",
    "#             log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "#             scheduler(epoch)\n",
    "\n",
    "#     model.eval()\n",
    "#     log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in dataset.test:\n",
    "#             inputs, targets = (b.to(device) for b in batch)\n",
    "#             predictions = model(inputs)\n",
    "#             loss = smooth_crossentropy(predictions, targets)\n",
    "#             correct = torch.argmax(predictions, 1) == targets\n",
    "#             log(model, loss.cpu(), correct.cpu())\n",
    "# log.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
