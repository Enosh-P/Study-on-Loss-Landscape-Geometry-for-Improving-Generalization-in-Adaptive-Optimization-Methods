{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed579354",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\enosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvgg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m VGG\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msmooth_cross_entropy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m smooth_crossentropy\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\__init__.py:126\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    124\u001B[0m     err \u001B[38;5;241m=\u001B[39m ctypes\u001B[38;5;241m.\u001B[39mWinError(last_error)\n\u001B[0;32m    125\u001B[0m     err\u001B[38;5;241m.\u001B[39mstrerror \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m Error loading \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdll\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m or one of its dependencies.\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    128\u001B[0m     is_loaded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mOSError\u001B[0m: [WinError 1455] The paging file is too small for this operation to complete. Error loading \"C:\\Users\\enosh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\cudnn_adv_infer64_8.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from model.vgg import VGG\n",
    "from model.smooth_cross_entropy import smooth_crossentropy\n",
    "from data.cifar import Cifar\n",
    "from utility.log import Log\n",
    "from utility.step_lr import StepLR\n",
    "from utility.bypass_bn import enable_running_stats, disable_running_stats\n",
    "\n",
    "import sys; sys.path.append(\"..\")\n",
    "from sam import SAM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ba42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive =  True\n",
    "batch_size = 128\n",
    "depth = 16\n",
    "dropout = 0.0\n",
    "epochs = 200\n",
    "label_smoothing = 0.1\n",
    "learning_rate = 0.001\n",
    "momentum= 0.9\n",
    "threads = 5\n",
    "rho = 2.0\n",
    "weight_decay = 0.0005\n",
    "width_factor = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517cf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(seed: int):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56531eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "initialize(seed=42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572949c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = Cifar(batch_size, threads)\n",
    "log = Log(log_each=10)\n",
    "model = VGG('VGG16').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5c1ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.Adam\n",
    "optimizer = SAM(model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_losses = np.zeros(epochs)\n",
    "train_accuracy = np.zeros(epochs)\n",
    "val_losses = np.zeros(epochs)\n",
    "val_accuracy = np.zeros(epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae48d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━┳━━━━━━━╸T╺╸R╺╸A╺╸I╺╸N╺━━━━━━━┳━━━━━━━╸S╺╸T╺╸A╺╸T╺╸S╺━━━━━━━┳━━━━━━━╸V╺╸A╺╸L╺╸I╺╸D╺━━━━━━━┓\n",
      "┃              ┃              ╷              ┃              ╷              ┃              ╷              ┃\n",
      "┃       epoch  ┃        loss  │    accuracy  ┃        l.r.  │     elapsed  ┃        loss  │    accuracy  ┃\n",
      "┠──────────────╂──────────────┼──────────────╂──────────────┼──────────────╂──────────────┼──────────────┨\n",
      "┃           0  ┃      1.1577  │     43.91 %  ┃   1.000e-03  │   01:15 min  ┠┈██████████████████┈┈┈┈┈┈┈┈┈┈┨"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    log.train(len_dataset=len(dataset.train))\n",
    "\n",
    "    for batch in dataset.train:\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "        # first forward-backward step\n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        # second forward-backward step\n",
    "        disable_running_stats(model)\n",
    "        smooth_crossentropy(model(inputs), targets, smoothing=label_smoothing).mean().backward()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    cur_loss, cur_acc = log.eval(len_dataset=len(dataset.test))\n",
    "    train_losses[epoch] = cur_loss\n",
    "    train_accuracy[epoch] = cur_acc\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu())\n",
    "            if epoch > 0:\n",
    "                val_losses[epoch-1] = log.current_valid_loss\n",
    "                val_accuracy[epoch-1] = log.current_valid_accuracy\n",
    "\n",
    "val_losses[epochs-1] = log.current_valid_loss\n",
    "val_accuracy[epochs-1] = log.current_valid_accuracy\n",
    "log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df7f3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45744c6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mZeroDivisionError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m      2\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mlog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlen_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m dataset\u001B[38;5;241m.\u001B[39mtrain:\n\u001B[0;32m      6\u001B[0m         inputs, targets \u001B[38;5;241m=\u001B[39m (b\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch)\n",
      "File \u001B[1;32mD:\\mini-project\\sam-main\\example\\utility\\log.py:17\u001B[0m, in \u001B[0;36mLog.train\u001B[1;34m(self, len_dataset)\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_print_header()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 17\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflush\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_train \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_steps_state \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m}\n",
      "File \u001B[1;32mD:\\mini-project\\sam-main\\example\\utility\\log.py:36\u001B[0m, in \u001B[0;36mLog.flush\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflush\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_train:\n\u001B[1;32m---> 36\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mloss\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_state\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msteps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m     37\u001B[0m         accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msteps\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m     40\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;124m┃\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m12d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  ┃\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m12.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  │\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m*\u001B[39maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m10.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m %  ┃\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m12.3e\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  │\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time()\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m>12\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m  ┃\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     41\u001B[0m             end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     42\u001B[0m             flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     43\u001B[0m         )\n",
      "\u001B[1;31mZeroDivisionError\u001B[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    log.train(len_dataset=len(dataset.train))\n",
    "\n",
    "    for batch in dataset.train:\n",
    "        inputs, targets = (b.to(device) for b in batch)\n",
    "        \n",
    "        enable_running_stats(model)\n",
    "        predictions = model(inputs)\n",
    "        loss = smooth_crossentropy(predictions, targets, smoothing=label_smoothing)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step(zero_grad=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = torch.argmax(predictions.data, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n",
    "            scheduler(epoch)\n",
    "\n",
    "    model.eval()\n",
    "    log.eval(len_dataset=len(dataset.test))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset.test:\n",
    "            inputs, targets = (b.to(device) for b in batch)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = smooth_crossentropy(predictions, targets)\n",
    "            correct = torch.argmax(predictions, 1) == targets\n",
    "            log(model, loss.cpu(), correct.cpu())\n",
    "\n",
    "log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2bfe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sam_model_adam_vgg.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), train_losses, label='Train loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation loss')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs + 1), train_accuracy, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracy, label='Train Accuracy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}